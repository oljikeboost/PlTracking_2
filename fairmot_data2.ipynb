{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from IPython.display import Video\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     9,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def get_color(lbl):\n",
    "\n",
    "    if lbl==0:\n",
    "        return (0,0,255)\n",
    "    elif lbl==1:\n",
    "        return (0,255,0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def post_process_cls(all_cls, all_tracks):\n",
    "\n",
    "    ### First, we need to get the set of all the tracks\n",
    "    ### After which, to find its corrsponding classes\n",
    "    ### And transform/interpolate the classes list\n",
    "    from collections import Counter\n",
    "    id_to_cls_list = {}\n",
    "    for en, (cls, track_id) in enumerate(zip(all_cls, all_tracks)):\n",
    "\n",
    "        if track_id in id_to_cls_list:\n",
    "            id_to_cls_list[track_id].append(cls)\n",
    "        else:\n",
    "            id_to_cls_list[track_id] = [cls]\n",
    "            \n",
    "\n",
    "    id_to_cls_val = {}\n",
    "    for track_id, cls_lst in id_to_cls_list.items():\n",
    "        cls_lst = np.array(cls_lst).flatten().tolist()\n",
    "        cnt = Counter(cls_lst)\n",
    "        mst_cmn = cnt.most_common()[0][0]\n",
    "        id_to_cls_val[track_id] = int(mst_cmn)\n",
    "\n",
    "    output = []\n",
    "    for en, track_id in enumerate(all_tracks):\n",
    "        \n",
    "        output.append(id_to_cls_val[track_id])\n",
    "\n",
    "    return output, id_to_cls_val\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)] \n",
    "\n",
    "def get_all_team_classes(id_dict):\n",
    "    print(\"Clustering all teams in progress...\")\n",
    "    anno_dirs = glob.glob('../data/third_task/*')\n",
    "    anno_dirs.extend(glob.glob('../data/raw_data/*'))\n",
    "    \n",
    "    ### Create global dict which maps global player track to its new global team class\n",
    "    global_id_to_cls_val = {}\n",
    "    all_cls = list(range(0, 2 * len(anno_dirs)))\n",
    "\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return [l[i:i+n] for i in range(0, len(l), n)]   \n",
    "    cls_chunks = chunks(all_cls, 2)  \n",
    "\n",
    "\n",
    "    for anno_en, anno_dir in enumerate(tqdm(anno_dirs)):\n",
    "        \n",
    "        ### Process a new game\n",
    "        all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "        orig_dir = os.path.join('../../data/playerTrackingFrames2', os.path.basename(anno_dir))\n",
    "        if not os.path.exists(orig_dir):\n",
    "            orig_dir = os.path.join('../../data/playerTrackingFrames', os.path.basename(anno_dir))\n",
    "        \n",
    "        \n",
    "        ### Create the corresponding history of labels and histograms\n",
    "        all_hists = []\n",
    "        all_labels = []\n",
    "        \n",
    "        anno_error = 0\n",
    "        box_cnt = 0\n",
    "        for en, single_json in enumerate(all_jsons):\n",
    "            data = json.load(open(single_json))\n",
    "            \n",
    "            \n",
    "            img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "            img0 = cv2.imread(img_path)\n",
    "            h,w,_ = img0.shape\n",
    "\n",
    "            for i in range(len(data['shapes'])):\n",
    "                box_cnt += 1\n",
    "            \n",
    "                label = data['shapes'][i]['label']\n",
    "                if '_' in label: continue\n",
    "                \n",
    "                pts = np.array(data['shapes'][i]['points']).astype(int)\n",
    "                if pts[0][1] > pts[1][1] or pts[0][0] > pts[1][0]: \n",
    "                    anno_error += 1\n",
    "                    continue\n",
    "\n",
    "                player_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "                \n",
    "                center_y = int((pts[1][1] + pts[0][1]) / 2)\n",
    "                center_x = int((pts[1][0] + pts[0][0]) / 2)\n",
    "\n",
    "                img_box = img0[max(0,center_y - 30): min(h, center_y + 30), \n",
    "                               max(0, center_x - 10): min(w, center_x + 10), :]\n",
    "\n",
    "                img_box = cv2.cvtColor(img_box, cv2.COLOR_BGR2HSV)\n",
    "                hist = cv2.calcHist([img_box], [0], None, [24],\n",
    "                                [0, 300])\n",
    "                hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "                all_hists.append(hist)\n",
    "                all_labels.append(player_label)\n",
    "                \n",
    "                \n",
    "\n",
    "        concat_hists = np.concatenate(all_hists)\n",
    "        km = KMeans(n_clusters=2, init=\"k-means++\", max_iter=10000).fit(all_hists)\n",
    "        proc_cls, id_to_cls_val = post_process_cls(km.labels_, all_labels)\n",
    "        \n",
    "#         print(anno_en, anno_dir, Counter(proc_cls), 100 * (anno_error/box_cnt))\n",
    "        \n",
    "        for player_id, color_cls in id_to_cls_val.items():\n",
    "            curr_cls_subset = cls_chunks[anno_en]\n",
    "            global_id_to_cls_val[player_id] = curr_cls_subset[color_cls]\n",
    "    \n",
    "    print('Clustering is finished!')\n",
    "    return proc_cls, global_id_to_cls_val                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dict which maps player ID from a game to its unique ID in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of class is  500\n",
      "The number of dirs is  50\n"
     ]
    }
   ],
   "source": [
    "anno_dirs = glob.glob('../data/third_task/*')\n",
    "anno_dirs.extend(glob.glob('../data/raw_data/*'))\n",
    "\n",
    "id_dict = {}\n",
    "k_class = 1\n",
    "for anno_dir in anno_dirs:\n",
    "    id_dict[os.path.basename(anno_dir)] = {}\n",
    "    \n",
    "    curr_set = set()\n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "    for single_json in all_jsons:\n",
    "        data = json.load(open(single_json))\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            if '_' not in data['shapes'][i]['label']:\n",
    "                curr_set.add(data['shapes'][i]['label'])\n",
    "            \n",
    "    num_classes = len(curr_set)\n",
    "    curr_classes = sorted(list(curr_set))\n",
    "    \n",
    "    en = 0\n",
    "    while en<num_classes:\n",
    "        \n",
    "        id_dict[os.path.basename(anno_dir)][curr_classes[en]] = k_class\n",
    "        en += 1\n",
    "        k_class += 1\n",
    "        \n",
    "print(\"The number of class is \", k_class)\n",
    "print(\"The number of dirs is \", len(anno_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gt.txt for each video. IMPORTANT! Frames should start from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create gt.txt with ball pocession information and jersey number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering all teams in progress...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59979ce611e6404ea00983367d62de14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering is finished!\n"
     ]
    }
   ],
   "source": [
    "_, id_to_cls_val = get_all_team_classes(id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0328b0c3dc647b7ad19c55870b8d5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_mot_third_task(id_dict, id_to_cls_val):\n",
    "    gt_list = []\n",
    "    anno_dirs = glob.glob('../data/third_task/*')\n",
    "\n",
    "\n",
    "    for anno_dir in tqdm(anno_dirs):\n",
    "\n",
    "        all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "\n",
    "\n",
    "        ### Iterate through all frames of current directory\n",
    "        cls_en = 0\n",
    "        gt_list = []\n",
    "        curr_labels = set()\n",
    "        for en, single_json in enumerate(all_jsons):\n",
    "            data = json.load(open(single_json))\n",
    "\n",
    "\n",
    "            ### The following block of code creates the jersey_dict which maps track_id to [jersey_num, ball_possession]\n",
    "            jersey_dict = {}                      \n",
    "            for i in range(len(data['shapes'])):\n",
    "\n",
    "                label = data['shapes'][i]['label']\n",
    "                if '_' not in label: continue\n",
    "\n",
    "                lbl_split = label.split('_')\n",
    "                if 'j' in label:\n",
    "                    _, track_id, jersey_num = lbl_split\n",
    "\n",
    "                    if not track_id in jersey_dict:\n",
    "                        jersey_dict[str(track_id)] = [jersey_num, 0]\n",
    "                    else:\n",
    "                        jersey_dict[str(track_id)][0] = jersey_num\n",
    "\n",
    "                elif 'b' in label:\n",
    "\n",
    "                    _, track_id = lbl_split\n",
    "\n",
    "                    if not track_id in jersey_dict:\n",
    "                        jersey_dict[track_id] = [None, 1]\n",
    "                    else:\n",
    "                        jersey_dict[track_id][1] = 1\n",
    "\n",
    "\n",
    "            for i in range(len(data['shapes'])):\n",
    "                bbox = data['shapes'][i]['points']  \n",
    "                label = data['shapes'][i]['label']\n",
    "                if '_' in label: continue\n",
    "\n",
    "                curr_labels.add(label)\n",
    "\n",
    "                if bbox[0][0] > bbox[1][0] or bbox[0][1] > bbox[1][1]: \n",
    "                    continue\n",
    "\n",
    "                track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "                team_lbl = id_to_cls_val[track_label]\n",
    "\n",
    "                jersey_num, ball_poc = jersey_dict.get(label, [None, 0])\n",
    "\n",
    "                anno_line = [en+1, track_label, \n",
    "                             int(bbox[0][0]), int(bbox[0][1]), \n",
    "                             int(bbox[1][0] - bbox[0][0]), int(bbox[1][1] - bbox[0][1]),\n",
    "                             1, 1, team_lbl, ball_poc]\n",
    "\n",
    "                anno_str = ','.join([str(x) for x in anno_line])     \n",
    "\n",
    "                gt_list.append(anno_str)\n",
    "\n",
    "\n",
    "\n",
    "        ### Create the output GT dir\n",
    "        output_dir = os.path.join('../data/mot_data/images/train/', os.path.basename(anno_dir))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_dir = os.path.join(output_dir, 'gt')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "        ### Write the detection to the file gt.txt\n",
    "        with open(os.path.join(output_dir, 'gt.txt'), 'w') as f:\n",
    "            for x in gt_list:\n",
    "                f.writelines(x + '\\n')       \n",
    "\n",
    "create_mot_third_task(id_dict, id_to_cls_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866a0ebff68b486d8bcb36f64564c9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_mot_first_second_task(id_dict, id_to_cls_val):\n",
    "    \n",
    "    gt_list = []\n",
    "    anno_dirs = glob.glob('../data/raw_data/*')\n",
    "    jersey_dir = '../data/second_task/'\n",
    "\n",
    "    for dr_en, anno_dir in enumerate(tqdm(anno_dirs)):\n",
    "\n",
    "        jersey_anno = os.path.join(jersey_dir, os.path.basename(anno_dir))\n",
    "\n",
    "        all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "\n",
    "\n",
    "        ### Iterate through all frames of current directory\n",
    "        cls_en = 0\n",
    "        gt_list = []\n",
    "        curr_labels = set()\n",
    "        for en, single_json in enumerate(all_jsons):\n",
    "            data = json.load(open(single_json))\n",
    "\n",
    "            jersey_file = os.path.join(jersey_anno, os.path.basename(single_json).replace('frame_', ''))\n",
    "\n",
    "            if os.path.exists(jersey_file):\n",
    "                jersey_data = json.load(open(jersey_file))   \n",
    "\n",
    "                ### Map each track for current frame to its existing information, such as Ball Pocession, Jersey Number, Position on Court\n",
    "                jersey_dict = {}\n",
    "                for i in range(len(jersey_data['shapes'])):\n",
    "                    bbox = jersey_data['shapes'][i]['points']  \n",
    "                    label = jersey_data['shapes'][i]['label']\n",
    "\n",
    "                    lbl_split = label.split('_')\n",
    "                    if 'j' in label:\n",
    "\n",
    "                        _, track_id, jersey_num = lbl_split\n",
    "\n",
    "\n",
    "                        if not track_id in jersey_dict:\n",
    "                            jersey_dict[str(track_id)] = [jersey_num, 0]\n",
    "                        else:\n",
    "                            jersey_dict[str(track_id)][0] = jersey_num\n",
    "\n",
    "                    elif 'b' in label:\n",
    "\n",
    "                        _, track_id = lbl_split\n",
    "\n",
    "                        if not track_id in jersey_dict:\n",
    "                            jersey_dict[track_id] = [None, 1]\n",
    "                        else:\n",
    "                            jersey_dict[track_id][1] = 1\n",
    "\n",
    "\n",
    "            for i in range(len(data['shapes'])):\n",
    "                bbox = data['shapes'][i]['points']  \n",
    "                label = data['shapes'][i]['label']\n",
    "                curr_labels.add(label)\n",
    "\n",
    "                if bbox[0][0] > bbox[1][0] or bbox[0][1] > bbox[1][1]: \n",
    "                    continue\n",
    "\n",
    "                track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "                team_lbl = id_to_cls_val[track_label]\n",
    "\n",
    "                if os.path.exists(jersey_file):\n",
    "                    jersey_num, ball_poc = jersey_dict.get(label, [None, 0])\n",
    "\n",
    "                anno_line = [en+1, track_label, \n",
    "                             int(bbox[0][0]), int(bbox[0][1]), \n",
    "                             int(bbox[1][0] - bbox[0][0]), int(bbox[1][1] - bbox[0][1]),\n",
    "                             1, 1, team_lbl, ball_poc]\n",
    "\n",
    "                anno_str = ','.join([str(x) for x in anno_line])     \n",
    "\n",
    "                gt_list.append(anno_str)\n",
    "\n",
    "\n",
    "\n",
    "        ### Create the output GT dir\n",
    "        output_dir = os.path.join('../data/mot_data/images/train/', os.path.basename(anno_dir))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_dir = os.path.join(output_dir, 'gt')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "        ### Write the detection to the file gt.txt\n",
    "        with open(os.path.join(output_dir, 'gt.txt'), 'w') as f:\n",
    "            for x in gt_list:\n",
    "                f.writelines(x + '\\n')   \n",
    "                \n",
    "create_mot_first_second_task(id_dict, id_to_cls_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy frames to mot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To understand which frame to copy, we need to create the set of all available frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "anno_dirs = glob.glob('../data/third_task/*')\n",
    "anno_dirs.extend(glob.glob('../data/raw_data/*'))\n",
    "\n",
    "set_of_all = set()\n",
    "for anno_dir in anno_dirs:\n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "    for js in all_jsons:\n",
    "        x = '/'.join(js.split('/')[-2:])\n",
    "        set_of_all.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "orig_frames = os.listdir('../../data/playerTrackingFrames')\n",
    "orig_frames.extend(os.listdir('../../data/playerTrackingFrames2'))\n",
    "\n",
    "\n",
    "for dr in all_dirs:\n",
    "    \n",
    "    if os.path.basename(dr) in orig_frames:\n",
    "        orig_dir = os.path.join('../../data/playerTrackingFrames', os.path.basename(dr))\n",
    "        if not os.path.exists(orig_dir):\n",
    "            orig_dir = os.path.join('../../data/playerTrackingFrames2', os.path.basename(dr))\n",
    "\n",
    "        dest_dir = os.path.join(dr, 'img1')\n",
    "\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "            os.makedirs(dest_dir)\n",
    "        else:\n",
    "            os.makedirs(dest_dir)\n",
    "        \n",
    "        curr_imgs = glob.glob(orig_dir + '/*.jpg')\n",
    "        for img in curr_imgs:\n",
    "            x = '/'.join(img.split('/')[-2:]).replace('.jpg', '.json')\n",
    "            if x in set_of_all:\n",
    "                shutil.copy2(img, dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename image files. IMPORTANT! Frames should start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "\n",
    "for dr in all_dirs:\n",
    "    img_dr = os.path.join(dr, 'img1')\n",
    "    curr_imgs = sorted(glob.glob(img_dr + '/*.jpg'))\n",
    "    \n",
    "    for en, img_path in enumerate(curr_imgs):\n",
    "        base = os.path.basename(img_path)\n",
    "        new_base = f\"{en+1:06d}.jpg\"\n",
    "        os.rename(img_path, img_path.replace(base, new_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom.train file, don't know why :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 5\n",
      "8223\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "all_dirs = sorted(all_dirs)\n",
    "\n",
    "\n",
    "# train_dirs = all_dirs[:int(0.9*len(all_dirs))]\n",
    "# val_dirs = all_dirs[int(0.9*len(all_dirs)):]\n",
    "# print(len(train_dirs), len(val_dirs))\n",
    "seqs_str = '''\n",
    "            2020.02.22-Michigan_at_Purdue,\n",
    "            2020.02.25-NorthCarolinaState_at_NorthCarolina,\n",
    "            2020.02.20-Oregon_at_ArizonaState,\n",
    "            2020.02.15-NotreDame_at_Duke,\n",
    "            UCLA vs Washington 2-15-20\n",
    "            '''\n",
    "data_root = '/home/ubuntu/oljike/PlayerTracking/data/mot_data/images/train'\n",
    "val_dirs = [seq.strip() for seq in seqs_str.split(',') if seq.strip()!='']\n",
    "val_dirs = [os.path.join('../data/mot_data/images/train/', x) for x in seqs]\n",
    "\n",
    "train_dirs = [x for x in all_dirs if x not in val_dirs]\n",
    "print(len(train_dirs), len(val_dirs))\n",
    "\n",
    "output = []\n",
    "for dr in train_dirs:\n",
    "    curr_files = sorted(glob.glob(dr + '/img1/*.jpg'))\n",
    "    for f in curr_files:\n",
    "        output.append(f.replace('../data/', ''))\n",
    "             \n",
    "with open('./src/data/custom.train', 'w') as f:\n",
    "    for l in output:\n",
    "        f.writelines(l + '\\n')\n",
    "        \n",
    "print(len(output))\n",
    "        \n",
    "\n",
    "output = []\n",
    "for dr in val_dirs:\n",
    "    curr_files = sorted(glob.glob(dr + '/img1/*.jpg'))\n",
    "    for f in curr_files:\n",
    "        output.append(f.replace('../data/', ''))\n",
    "             \n",
    "with open('./src/data/custom.val', 'w') as f:\n",
    "    for l in output:\n",
    "        f.writelines(l + '\\n')\n",
    "        \n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "cfg['root'] = '/home/ubuntu/oljike/PlayerTracking/data'\n",
    "cfg['train'] = {}\n",
    "cfg['train']['custom'] = './data/custom.train'\n",
    "cfg['test'] = {}\n",
    "cfg['test']['custom'] = './data/custom.val'\n",
    "cfg['test_emb'] = './data/custom.val'\n",
    "\n",
    "\n",
    "with open('src/lib/cfg/custom.json','w') as f:\n",
    "    json.dump(cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create team color labels and check them visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_team_color_labels(anno_dir, all_jsons):\n",
    "    all_labels = []\n",
    "    all_hists = []\n",
    "\n",
    "    \n",
    "    orig_dir = os.path.join('../../data/player_tracking_frames', os.path.basename(anno_dir))\n",
    "    for single_json in all_jsons:\n",
    "        data = json.load(open(single_json))\n",
    "\n",
    "        ### Read the image\n",
    "        img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "        img0 = cv2.imread(img_path)\n",
    "        h,w,_ = img0.shape\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            pts = np.array(data['shapes'][i]['points']).astype(int)\n",
    "\n",
    "            if pts[0][1] > pts[1][1] or pts[0][0] > pts[1][0]: continue\n",
    "            center_y = int((pts[1][1] + pts[0][1]) / 2)\n",
    "            center_x = int((pts[1][0] + pts[0][0]) / 2)\n",
    "\n",
    "            img_box = img0[max(0,center_y - 30): min(h, center_y + 30), \n",
    "                           max(0, center_x - 10): min(w, center_x + 10), :]\n",
    "            cv2.imwrite('small.jpg', img_box)\n",
    "\n",
    "            img_box = cv2.cvtColor(img_box, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            hist = cv2.calcHist([img_box], [0], None, [24],\n",
    "                                [0, 300])\n",
    "            hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "            all_hists.append(hist)\n",
    "            all_labels.append(data['shapes'][i]['label'])\n",
    "\n",
    "    concat_hists = np.concatenate(all_hists)\n",
    "    print(hist.shape)\n",
    "    km = KMeans(n_clusters=2, init=\"k-means++\", max_iter=10000).fit(all_hists)\n",
    "    print(Counter(km.labels_))\n",
    "    proc_cls, id_to_cls_val = post_process_cls(km.labels_, all_labels)\n",
    "    print(Counter(proc_cls))\n",
    "    return proc_cls, id_to_cls_val\n",
    "\n",
    "# anno_dir = glob.glob('../data/raw_data/*')[24]\n",
    "# all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "# proc_cls, id_to_cls_val = get_team_color_labels(anno_dir, all_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "h, w, _ = 720, 1280, 0\n",
    "out = cv2.VideoWriter('team_label_output.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 3, (w,h))\n",
    "\n",
    "orig_dir = os.path.join('../../data/player_tracking_frames', os.path.basename(anno_dir))\n",
    "\n",
    "en = 0\n",
    "for single_json in all_jsons:\n",
    "    data = json.load(open(single_json))\n",
    "    \n",
    "    ### Read the image\n",
    "    img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    for shape in data['shapes']:\n",
    "        bbox = np.array([[int(x) for x in y] for y in shape['points']])\n",
    "        \n",
    "        bbox = bbox.flatten()\n",
    "   \n",
    "        color = get_color(id_to_cls_val[shape['label']])\n",
    "        \n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, thickness=1)\n",
    "        cv2.putText(img, shape['label'], (bbox[0], max(0, bbox[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), thickness=1)\n",
    "       \n",
    "        en += 1\n",
    "    out.write(img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('team_label_output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visual check\n",
    "h, w, _ = 720, 1280, 0\n",
    "out = cv2.VideoWriter('team_label_output.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 3, (w,h))\n",
    "\n",
    "\n",
    "anno_dir = glob.glob('../corrected/2019-01-12_Tennessee_at_Florida*')[0]\n",
    "all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "orig_dir = os.path.join('../../data/playerTrackingFrames/', os.path.basename(anno_dir))\n",
    "en = 0\n",
    "\n",
    "for single_json in all_jsons:\n",
    "    data = json.load(open(single_json))\n",
    "    \n",
    "    ### Read the image\n",
    "    img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    for shape in data['shapes']:\n",
    "#         if '_' in shape['label']: continue\n",
    "        bbox = np.array([[int(x) for x in y] for y in shape['points']])\n",
    "        label = shape['label']\n",
    "        bbox = bbox.flatten()\n",
    "#         track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "#         player_lbl = id_to_cls_val[track_label]\n",
    "#         color = get_color(player_lbl)\n",
    "        \n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255,0,0), thickness=1)\n",
    "        cv2.putText(img, str(label), (bbox[0], max(0, bbox[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), thickness=5)\n",
    "       \n",
    "        en += 1\n",
    "    out.write(img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"team_label_output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('team_label_output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
