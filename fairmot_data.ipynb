{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from IPython.display import Video\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     9,
     38,
     42
    ]
   },
   "outputs": [],
   "source": [
    "def get_color(lbl):\n",
    "\n",
    "    if lbl==0:\n",
    "        return (0,0,255)\n",
    "    elif lbl==1:\n",
    "        return (0,255,0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def post_process_cls(all_cls, all_tracks):\n",
    "\n",
    "    ### First, we need to get the set of all the tracks\n",
    "    ### After which, to find its corrsponding classes\n",
    "    ### And transform/interpolate the classes list\n",
    "    from collections import Counter\n",
    "    id_to_cls_list = {}\n",
    "    for en, (cls, track_id) in enumerate(zip(all_cls, all_tracks)):\n",
    "\n",
    "        if track_id in id_to_cls_list:\n",
    "            id_to_cls_list[track_id].append(cls)\n",
    "        else:\n",
    "            id_to_cls_list[track_id] = [cls]\n",
    "            \n",
    "\n",
    "    id_to_cls_val = {}\n",
    "    for track_id, cls_lst in id_to_cls_list.items():\n",
    "        cls_lst = np.array(cls_lst).flatten().tolist()\n",
    "        cnt = Counter(cls_lst)\n",
    "        mst_cmn = cnt.most_common()[0][0]\n",
    "        id_to_cls_val[track_id] = int(mst_cmn)\n",
    "\n",
    "    output = []\n",
    "    for en, track_id in enumerate(all_tracks):\n",
    "        \n",
    "        output.append(id_to_cls_val[track_id])\n",
    "\n",
    "    return output, id_to_cls_val\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)] \n",
    "\n",
    "def get_all_team_classes(id_dict):\n",
    "    print(\"Clustering all teams in progress...\")\n",
    "    anno_dirs = glob.glob('../data/raw_data/*')\n",
    "    \n",
    "    ### Create global dict which maps global player track to its new global team class\n",
    "    global_id_to_cls_val = {}\n",
    "    all_cls = list(range(0, 2 * len(anno_dirs)))\n",
    "\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return [l[i:i+n] for i in range(0, len(l), n)]   \n",
    "    cls_chunks = chunks(all_cls, 2)  \n",
    "\n",
    "\n",
    "    for anno_en, anno_dir in enumerate(tqdm(anno_dirs)):\n",
    "        \n",
    "        ### Process a new game\n",
    "        all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "        orig_dir = os.path.join('../../data/playerTrackingFrames', os.path.basename(anno_dir))\n",
    "        \n",
    "        ### Create the corresponding history of labels and histograms\n",
    "        all_hists = []\n",
    "        all_labels = []\n",
    "        \n",
    "        anno_error = 0\n",
    "        box_cnt = 0\n",
    "        for en, single_json in enumerate(all_jsons):\n",
    "            data = json.load(open(single_json))\n",
    "            \n",
    "            \n",
    "            img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "            img0 = cv2.imread(img_path)\n",
    "            h,w,_ = img0.shape\n",
    "\n",
    "            for i in range(len(data['shapes'])):\n",
    "                box_cnt += 1\n",
    "            \n",
    "                label = data['shapes'][i]['label']\n",
    "                pts = np.array(data['shapes'][i]['points']).astype(int)\n",
    "                if pts[0][1] > pts[1][1] or pts[0][0] > pts[1][0]: \n",
    "                    anno_error += 1\n",
    "                    continue\n",
    "\n",
    "                player_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "                \n",
    "                center_y = int((pts[1][1] + pts[0][1]) / 2)\n",
    "                center_x = int((pts[1][0] + pts[0][0]) / 2)\n",
    "\n",
    "                img_box = img0[max(0,center_y - 30): min(h, center_y + 30), \n",
    "                               max(0, center_x - 10): min(w, center_x + 10), :]\n",
    "\n",
    "                img_box = cv2.cvtColor(img_box, cv2.COLOR_BGR2HSV)\n",
    "                hist = cv2.calcHist([img_box], [0], None, [24],\n",
    "                                [0, 300])\n",
    "                hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "                all_hists.append(hist)\n",
    "                all_labels.append(player_label)\n",
    "                \n",
    "                \n",
    "\n",
    "        concat_hists = np.concatenate(all_hists)\n",
    "        km = KMeans(n_clusters=2, init=\"k-means++\", max_iter=10000).fit(all_hists)\n",
    "        proc_cls, id_to_cls_val = post_process_cls(km.labels_, all_labels)\n",
    "        \n",
    "        print(anno_en, anno_dir, Counter(proc_cls), 100 * (anno_error/box_cnt))\n",
    "        \n",
    "        for player_id, color_cls in id_to_cls_val.items():\n",
    "            curr_cls_subset = cls_chunks[anno_en]\n",
    "            global_id_to_cls_val[player_id] = curr_cls_subset[color_cls]\n",
    "    \n",
    "    print('Clustering is finished!')\n",
    "    return proc_cls, global_id_to_cls_val                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dict which maps player ID from a game to its unique ID in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of class is  410\n",
      "The number of dirs is  41\n"
     ]
    }
   ],
   "source": [
    "anno_dirs = glob.glob('../data/raw_data/*')\n",
    "\n",
    "id_dict = {}\n",
    "k_class = 1\n",
    "for anno_dir in anno_dirs:\n",
    "    id_dict[os.path.basename(anno_dir)] = {}\n",
    "    \n",
    "    curr_set = set()\n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "    for single_json in all_jsons:\n",
    "        data = json.load(open(single_json))\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            curr_set.add(data['shapes'][i]['label'])\n",
    "            \n",
    "    num_classes = len(curr_set)\n",
    "    curr_classes = sorted(list(curr_set))\n",
    "    \n",
    "    en = 0\n",
    "    while en<num_classes:\n",
    "        \n",
    "        id_dict[os.path.basename(anno_dir)][curr_classes[en]] = k_class\n",
    "        en += 1\n",
    "        k_class += 1\n",
    "        \n",
    "print(\"The number of class is \", k_class)\n",
    "print(\"The number of dirs is \", len(anno_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gt.txt for each video. IMPORTANT! Frames should start from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create gt.txt with team label having only {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# gt_list = []\n",
    "# anno_dirs = glob.glob('../data/raw_data/*')\n",
    "\n",
    "\n",
    "# for anno_dir in tqdm(anno_dirs):\n",
    "#     all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "    \n",
    "    \n",
    "#     team_cls, id_to_cls_val = get_team_color_labels(anno_dir, all_jsons)\n",
    "    \n",
    "#     gt_list = []\n",
    "#     cls_en = 0\n",
    "#     for en, single_json in enumerate(all_jsons):\n",
    "#         data = json.load(open(single_json))\n",
    "\n",
    "#         for i in range(len(data['shapes'])):\n",
    "#             bbox = data['shapes'][i]['points']  \n",
    "#             label = data['shapes'][i]['label']\n",
    "            \n",
    "#             player_lbl = id_to_cls_val[label]\n",
    "            \n",
    "#             if bbox[0][0] > bbox[1][0] or bbox[0][1] > bbox[1][1]: \n",
    "#                 print(\"BBOX ERROR\")\n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "#             anno_line = [en+1, id_dict[os.path.basename(anno_dir)][label], \n",
    "#                          int(bbox[0][0]), int(bbox[0][1]), \n",
    "#                          int(bbox[1][0] - bbox[0][0]), int(bbox[1][1] - bbox[0][1]),\n",
    "#                          1, 1, player_lbl]\n",
    "\n",
    "#             anno_str = ','.join([str(x) for x in anno_line])     \n",
    "\n",
    "#             gt_list.append(anno_str)\n",
    "           \n",
    "        \n",
    "#     ### Create the output GT dir\n",
    "#     output_dir = os.path.join('../data/mot_data/images/train/', os.path.basename(anno_dir))\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "#     output_dir = os.path.join(output_dir, 'gt')\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "#     ### Write the detection to the file gt.txt\n",
    "#     with open(os.path.join(output_dir, 'gt.txt'), 'w') as f:\n",
    "#         for x in gt_list:\n",
    "#             f.writelines(x + '\\n')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create gt.txt with all teams having its own label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering all teams in progress...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94df123415140cc81292ed3cb8dc506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../data/raw_data/2020.02.13-Colorado_at_Oregon Counter({1: 1000, 0: 982}) 0.15113350125944583\n",
      "1 ../data/raw_data/2018.11.27-Indiana_at_Duke Counter({1: 970, 0: 924}) 0.0\n",
      "2 ../data/raw_data/2019-02-11_Virginia_at_North_Carolina Counter({1: 1232, 0: 527}) 0.0\n",
      "3 ../data/raw_data/2020.02.22-Michigan_at_Purdue Counter({1: 1275, 0: 542}) 0.0\n",
      "4 ../data/raw_data/2019.01.22-Duke_at_Pittsburgh Counter({0: 1000, 1: 954}) 0.051150895140664954\n",
      "5 ../data/raw_data/2020.02.25-NorthCarolinaState_at_NorthCarolina Counter({1: 988, 0: 952}) 0.0\n",
      "6 ../data/raw_data/2019.02.26-Duke_at_VirginiaTech Counter({1: 1112, 0: 761}) 0.0\n",
      "7 ../data/raw_data/2019.03.14-ACC-Syracuse_at_Duke Counter({1: 882, 0: 842}) 0.0\n",
      "8 ../data/raw_data/2020.02.15-NotreDame_at_Duke Counter({1: 1005, 0: 730}) 0.0576036866359447\n",
      "9 ../data/raw_data/2020.02.04-Duke_at_BostonCollege Counter({0: 939, 1: 919}) 0.0\n",
      "10 ../data/raw_data/2020.02.10-FloridaState_at_Duke Counter({1: 976, 0: 711}) 0.0\n",
      "11 ../data/raw_data/2020-02-15-Virginia_at_NorthCarolina Counter({1: 954, 0: 636}) 0.0\n",
      "12 ../data/raw_data/2020.01.18-Louisville_at_Duke Counter({0: 864, 1: 864}) 0.0\n",
      "13 ../data/raw_data/2020.01.21-MiamiFL_at_Duke Counter({1: 1081, 0: 684}) 0.05662514156285391\n",
      "14 ../data/raw_data/2018-11-28_Virginia_at_Maryland Counter({1: 985, 0: 899}) 0.0\n",
      "15 ../data/raw_data/2020.02.01-Duke_at_Syracuse Counter({1: 837, 0: 661}) 0.0\n",
      "16 ../data/raw_data/2019.02.16-NorthCarolinaState_at_Duke Counter({1: 809, 0: 775}) 0.0\n",
      "17 ../data/raw_data/2019.02.09_Florida_at_Tennessee Counter({1: 119, 0: 80}) 0.0\n",
      "18 ../data/raw_data/2019.11.29-Oregon_at_NorthCarolina Counter({0: 491, 1: 480}) 0.0\n",
      "19 ../data/raw_data/2020.01.28-Pittsburgh_at_Duke Counter({1: 1248, 0: 516}) 0.0\n",
      "20 ../data/raw_data/2019-02-18_Virginia_at_VirginiaTech Counter({1: 858, 0: 823}) 0.0\n",
      "21 ../data/raw_data/2020.01.25_Baylor_at_Florida Counter({1: 1307, 0: 569}) 0.0\n",
      "22 ../data/raw_data/2020.01.07-Kentucky_at_Georgia Counter({1: 979, 0: 975}) 0.0\n",
      "23 ../data/raw_data/2020.01.27-NorthCarolina_at_NorthCarolinaState Counter({1: 891, 0: 822}) 0.0\n",
      "24 ../data/raw_data/2020.02.20-Oregon_at_ArizonaState Counter({1: 968, 0: 909}) 0.0\n",
      "25 ../data/raw_data/2019.11.28-NorthCarolina_at_Michigan Counter({1: 849, 0: 694}) 0.0\n",
      "26 ../data/raw_data/UCLA vs Washington 2-15-20 Counter({1: 980, 0: 832}) 0.0\n",
      "27 ../data/raw_data/2020.02.08-Duke_at_NorthCarolina Counter({0: 1154, 1: 788}) 0.0\n",
      "28 ../data/raw_data/2019.01.05-Clemson_at_Duke Counter({0: 922, 1: 913}) 0.0\n",
      "29 ../data/raw_data/2019-02-25_NotreDame_at_FloridaState Counter({0: 1295, 1: 537}) 0.0\n",
      "30 ../data/raw_data/2019-01-19_Virginia_at_Duke Counter({0: 1261, 1: 548}) 0.0\n",
      "31 ../data/raw_data/2019-03-09_Louisville_at_Virginia Counter({0: 963, 1: 880}) 0.0\n",
      "32 ../data/raw_data/2020.01.18-Kentucky_at_Arkansas Counter({0: 995, 1: 953}) 0.0\n",
      "33 ../data/raw_data/2020-01-11_Syracuse_at_Virginia Counter({1: 1334, 0: 585}) 0.0\n",
      "34 ../data/raw_data/2019.02.09_Duke_at_Virginia Counter({0: 155, 1: 121}) 0.0\n",
      "35 ../data/raw_data/2020.01.18-NorthCarolina_at_Pittsburgh Counter({0: 859, 1: 514}) 0.0\n",
      "36 ../data/raw_data/2019-03-04_Virginia_at_Syracuse Counter({0: 814, 1: 784}) 0.0\n",
      "37 ../data/raw_data/2020-02-06 UCLA @ ASU FULL Counter({1: 984, 0: 978}) 0.0\n",
      "38 ../data/raw_data/2020.01.04-Duke_at_MiamiFL Counter({0: 352, 1: 225}) 0.0\n",
      "39 ../data/raw_data/2019-01-12_Tennessee_at_Florida Counter({0: 960, 1: 933}) 0.0\n",
      "40 ../data/raw_data/2020.02.03-NorthCarolina_at_FloridaState Counter({1: 15974, 0: 9280}) 0.0\n",
      "\n",
      "Clustering is finished!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd6e473253c47d1b1e378dbdb51f5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt_list = []\n",
    "anno_dirs = glob.glob('../data/raw_data/*')\n",
    "\n",
    "_, id_to_cls_val = get_all_team_classes(id_dict)\n",
    "\n",
    "for anno_dir in tqdm(anno_dirs):\n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "\n",
    "    \n",
    "    gt_list = []\n",
    "    cls_en = 0\n",
    "    for en, single_json in enumerate(all_jsons):\n",
    "        data = json.load(open(single_json))\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            bbox = data['shapes'][i]['points']  \n",
    "            label = data['shapes'][i]['label']\n",
    "            \n",
    "            if bbox[0][0] > bbox[1][0] or bbox[0][1] > bbox[1][1]: \n",
    "                continue\n",
    "                \n",
    "            track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "            player_lbl = id_to_cls_val[track_label]\n",
    "            \n",
    "            \n",
    "            anno_line = [en+1, track_label, \n",
    "                         int(bbox[0][0]), int(bbox[0][1]), \n",
    "                         int(bbox[1][0] - bbox[0][0]), int(bbox[1][1] - bbox[0][1]),\n",
    "                         1, 1, player_lbl]\n",
    "\n",
    "            anno_str = ','.join([str(x) for x in anno_line])     \n",
    "\n",
    "            gt_list.append(anno_str)\n",
    "           \n",
    "        \n",
    "    ### Create the output GT dir\n",
    "    output_dir = os.path.join('../data/mot_data/images/train/', os.path.basename(anno_dir))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_dir = os.path.join(output_dir, 'gt')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    ### Write the detection to the file gt.txt\n",
    "    with open(os.path.join(output_dir, 'gt.txt'), 'w') as f:\n",
    "        for x in gt_list:\n",
    "            f.writelines(x + '\\n')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create gt.txt with ball pocession information and jersey number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering all teams in progress...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e136219f1fb4ab5b6a371b506548606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../data/raw_data/2020.02.13-Colorado_at_Oregon Counter({0: 1000, 1: 982}) 0.15113350125944583\n",
      "1 ../data/raw_data/2018.11.27-Indiana_at_Duke Counter({0: 970, 1: 924}) 0.0\n",
      "2 ../data/raw_data/2019-02-11_Virginia_at_North_Carolina Counter({0: 1232, 1: 527}) 0.0\n",
      "3 ../data/raw_data/2020.02.22-Michigan_at_Purdue Counter({1: 1275, 0: 542}) 0.0\n",
      "4 ../data/raw_data/2019.01.22-Duke_at_Pittsburgh Counter({1: 1000, 0: 954}) 0.051150895140664954\n",
      "5 ../data/raw_data/2020.02.25-NorthCarolinaState_at_NorthCarolina Counter({1: 988, 0: 952}) 0.0\n",
      "6 ../data/raw_data/2019.02.26-Duke_at_VirginiaTech Counter({1: 1112, 0: 761}) 0.0\n",
      "7 ../data/raw_data/2019.03.14-ACC-Syracuse_at_Duke Counter({0: 882, 1: 842}) 0.0\n",
      "8 ../data/raw_data/2020.02.15-NotreDame_at_Duke Counter({1: 1005, 0: 730}) 0.0576036866359447\n",
      "9 ../data/raw_data/2020.02.04-Duke_at_BostonCollege Counter({0: 939, 1: 919}) 0.0\n",
      "10 ../data/raw_data/2020.02.10-FloridaState_at_Duke Counter({0: 976, 1: 711}) 0.0\n",
      "11 ../data/raw_data/2020-02-15-Virginia_at_NorthCarolina Counter({0: 954, 1: 636}) 0.0\n",
      "12 ../data/raw_data/2020.01.18-Louisville_at_Duke Counter({1: 864, 0: 864}) 0.0\n",
      "13 ../data/raw_data/2020.01.21-MiamiFL_at_Duke Counter({0: 1081, 1: 684}) 0.05662514156285391\n",
      "14 ../data/raw_data/2018-11-28_Virginia_at_Maryland Counter({0: 985, 1: 899}) 0.0\n",
      "15 ../data/raw_data/2020.02.01-Duke_at_Syracuse Counter({1: 837, 0: 661}) 0.0\n",
      "16 ../data/raw_data/2019.02.16-NorthCarolinaState_at_Duke Counter({1: 809, 0: 775}) 0.0\n",
      "17 ../data/raw_data/2019.02.09_Florida_at_Tennessee Counter({0: 119, 1: 80}) 0.0\n",
      "18 ../data/raw_data/2019.11.29-Oregon_at_NorthCarolina Counter({1: 491, 0: 480}) 0.0\n",
      "19 ../data/raw_data/2020.01.28-Pittsburgh_at_Duke Counter({0: 1248, 1: 516}) 0.0\n",
      "20 ../data/raw_data/2019-02-18_Virginia_at_VirginiaTech Counter({1: 858, 0: 823}) 0.0\n",
      "21 ../data/raw_data/2020.01.25_Baylor_at_Florida Counter({1: 1307, 0: 569}) 0.0\n",
      "22 ../data/raw_data/2020.01.07-Kentucky_at_Georgia Counter({1: 979, 0: 975}) 0.0\n",
      "23 ../data/raw_data/2020.01.27-NorthCarolina_at_NorthCarolinaState Counter({1: 891, 0: 822}) 0.0\n",
      "24 ../data/raw_data/2020.02.20-Oregon_at_ArizonaState Counter({1: 968, 0: 909}) 0.0\n",
      "25 ../data/raw_data/2019.11.28-NorthCarolina_at_Michigan Counter({1: 849, 0: 694}) 0.0\n",
      "26 ../data/raw_data/UCLA vs Washington 2-15-20 Counter({0: 980, 1: 832}) 0.0\n",
      "27 ../data/raw_data/2020.02.08-Duke_at_NorthCarolina Counter({1: 1154, 0: 788}) 0.0\n",
      "28 ../data/raw_data/2019.01.05-Clemson_at_Duke Counter({1: 922, 0: 913}) 0.0\n",
      "29 ../data/raw_data/2019-02-25_NotreDame_at_FloridaState Counter({1: 1295, 0: 537}) 0.0\n",
      "30 ../data/raw_data/2019-01-19_Virginia_at_Duke Counter({1: 1261, 0: 548}) 0.0\n",
      "31 ../data/raw_data/2019-03-09_Louisville_at_Virginia Counter({0: 963, 1: 880}) 0.0\n",
      "32 ../data/raw_data/2020.01.18-Kentucky_at_Arkansas Counter({1: 995, 0: 953}) 0.0\n",
      "33 ../data/raw_data/2020-01-11_Syracuse_at_Virginia Counter({1: 1334, 0: 585}) 0.0\n",
      "34 ../data/raw_data/2019.02.09_Duke_at_Virginia Counter({1: 155, 0: 121}) 0.0\n",
      "35 ../data/raw_data/2020.01.18-NorthCarolina_at_Pittsburgh Counter({0: 859, 1: 514}) 0.0\n",
      "36 ../data/raw_data/2019-03-04_Virginia_at_Syracuse Counter({1: 814, 0: 784}) 0.0\n",
      "37 ../data/raw_data/2020-02-06 UCLA @ ASU FULL Counter({0: 984, 1: 978}) 0.0\n",
      "38 ../data/raw_data/2020.01.04-Duke_at_MiamiFL Counter({0: 352, 1: 225}) 0.0\n",
      "39 ../data/raw_data/2019-01-12_Tennessee_at_Florida Counter({1: 960, 0: 933}) 0.0\n",
      "40 ../data/raw_data/2020.02.03-NorthCarolina_at_FloridaState Counter({1: 15974, 0: 9280}) 0.0\n",
      "\n",
      "Clustering is finished!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3dfaebdae54d52a21206ac7d22c344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt_list = []\n",
    "anno_dirs = glob.glob('../data/raw_data/*')\n",
    "jersey_dir = '../data/second_task/'\n",
    "\n",
    "\n",
    "_, id_to_cls_val = get_all_team_classes(id_dict)\n",
    "\n",
    "for anno_dir in tqdm(anno_dirs):\n",
    "    \n",
    "    jersey_anno = os.path.join(jersey_dir, os.path.basename(anno_dir))\n",
    "    \n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "\n",
    "    \n",
    "    ### Iterate through all frames of current directory\n",
    "    cls_en = 0\n",
    "    gt_list = []\n",
    "    curr_labels = set()\n",
    "    for en, single_json in enumerate(all_jsons):\n",
    "        data = json.load(open(single_json))\n",
    "        \n",
    "        jersey_file = os.path.join(jersey_anno, os.path.basename(single_json).replace('frame_', ''))\n",
    "        \n",
    "        if os.path.exists(jersey_file):\n",
    "            jersey_data = json.load(open(jersey_file))   \n",
    "        \n",
    "            ### Map each track for current frame to its existing information, such as Ball Pocession, Jersey Number, Position on Court\n",
    "            jersey_dict = {}\n",
    "            for i in range(len(jersey_data['shapes'])):\n",
    "                bbox = jersey_data['shapes'][i]['points']  \n",
    "                label = jersey_data['shapes'][i]['label']\n",
    "\n",
    "                lbl_split = label.split('_')\n",
    "                if 'j' in label:\n",
    "\n",
    "                    if len(lbl_split)==4:\n",
    "                        jersey_num = lbl_split[-1]\n",
    "                        track_id  = '_'.join([lbl_split[1], lbl_split[2]])\n",
    "                    else:\n",
    "                        _, track_id, jersey_num = lbl_split\n",
    "\n",
    "\n",
    "                    if not track_id in jersey_dict:\n",
    "                        jersey_dict[str(track_id)] = [jersey_num, 0]\n",
    "                    else:\n",
    "                        jersey_dict[str(track_id)][0] = jersey_num\n",
    "\n",
    "                elif 'b' in label:\n",
    "                    \n",
    "                    if len(lbl_split)==3:\n",
    "                        track_id  = '_'.join([lbl_split[1], lbl_split[2]])\n",
    "                    else:\n",
    "                        _, track_id = lbl_split\n",
    "                        \n",
    "                    if not track_id in jersey_dict:\n",
    "                        jersey_dict[track_id] = [None, 1]\n",
    "                    else:\n",
    "                        jersey_dict[track_id][1] = 1\n",
    "\n",
    "                        \n",
    "        for i in range(len(data['shapes'])):\n",
    "            bbox = data['shapes'][i]['points']  \n",
    "            label = data['shapes'][i]['label']\n",
    "            curr_labels.add(label)\n",
    "            \n",
    "            if bbox[0][0] > bbox[1][0] or bbox[0][1] > bbox[1][1]: \n",
    "                continue\n",
    "                \n",
    "            track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "            team_lbl = id_to_cls_val[track_label]\n",
    "            \n",
    "            if os.path.exists(jersey_file):\n",
    "                jersey_num, ball_poc = jersey_dict.get(label, [None, 0])\n",
    "            \n",
    "            anno_line = [en+1, track_label, \n",
    "                         int(bbox[0][0]), int(bbox[0][1]), \n",
    "                         int(bbox[1][0] - bbox[0][0]), int(bbox[1][1] - bbox[0][1]),\n",
    "                         1, 1, team_lbl, ball_poc]\n",
    "\n",
    "            anno_str = ','.join([str(x) for x in anno_line])     \n",
    "\n",
    "            gt_list.append(anno_str)\n",
    "\n",
    "\n",
    "    \n",
    "    ### Create the output GT dir\n",
    "    output_dir = os.path.join('../data/mot_data/images/train/', os.path.basename(anno_dir))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_dir = os.path.join(output_dir, 'gt')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "    ### Write the detection to the file gt.txt\n",
    "    with open(os.path.join(output_dir, 'gt.txt'), 'w') as f:\n",
    "        for x in gt_list:\n",
    "            f.writelines(x + '\\n')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANNO FORMAT\n",
    "### frame_number obj_ID x y width height 1 (conf score) 1 (obj type) visibility_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy frames to mot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To understand which frame to copy, we need to create the set of all available frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "anno_dirs = glob.glob('../data/raw_data/*')\n",
    "\n",
    "set_of_all = set()\n",
    "for anno_dir in anno_dirs:\n",
    "    all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "    for js in all_jsons:\n",
    "        x = '/'.join(js.split('/')[-2:])\n",
    "        set_of_all.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "orig_frames = os.listdir('../../data/playerTrackingFrames')\n",
    "\n",
    "\n",
    "for dr in all_dirs:\n",
    "    \n",
    "    if os.path.basename(dr) in orig_frames:\n",
    "        orig_dir = os.path.join('../../data/playerTrackingFrames', os.path.basename(dr))\n",
    "\n",
    "        dest_dir = os.path.join(dr, 'img1')\n",
    "        \n",
    "        \n",
    "        if os.path.exists(dest_dir):\n",
    "            if not os.path.isdir(dest_dir):\n",
    "                os.remove(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "orig_frames = os.listdir('../../data/playerTrackingFrames')\n",
    "\n",
    "\n",
    "for dr in all_dirs:\n",
    "    \n",
    "    if os.path.basename(dr) in orig_frames:\n",
    "        orig_dir = os.path.join('../../data/playerTrackingFrames', os.path.basename(dr))\n",
    "\n",
    "        dest_dir = os.path.join(dr, 'img1')\n",
    "\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "            os.makedirs(dest_dir)\n",
    "        else:\n",
    "            os.makedirs(dest_dir)\n",
    "        \n",
    "        curr_imgs = glob.glob(orig_dir + '/*.jpg')\n",
    "        for img in curr_imgs:\n",
    "            x = '/'.join(img.split('/')[-2:]).replace('.jpg', '.json')\n",
    "            if x in set_of_all:\n",
    "                shutil.copy2(img, dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename image files. IMPORTANT! Frames should start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "\n",
    "for dr in all_dirs:\n",
    "    img_dr = os.path.join(dr, 'img1')\n",
    "    curr_imgs = sorted(glob.glob(img_dr + '/*.jpg'))\n",
    "    \n",
    "    for en, img_path in enumerate(curr_imgs):\n",
    "        base = os.path.basename(img_path)\n",
    "        new_base = f\"{en+1:06d}.jpg\"\n",
    "        os.rename(img_path, img_path.replace(base, new_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom.train file, don't know why :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 5\n",
      "6505\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "all_dirs = glob.glob('../data/mot_data/images/train/*')\n",
    "all_dirs = sorted(all_dirs)\n",
    "\n",
    "\n",
    "train_dirs = all_dirs[:int(0.9*len(all_dirs))]\n",
    "val_dirs = all_dirs[int(0.9*len(all_dirs)):]\n",
    "print(len(train_dirs), len(val_dirs))\n",
    "\n",
    "output = []\n",
    "for dr in train_dirs:\n",
    "    curr_files = sorted(glob.glob(dr + '/img1/*.jpg'))\n",
    "    for f in curr_files:\n",
    "        output.append(f.replace('../data/', ''))\n",
    "             \n",
    "with open('./src/data/custom.train', 'w') as f:\n",
    "    for l in output:\n",
    "        f.writelines(l + '\\n')\n",
    "        \n",
    "print(len(output))\n",
    "        \n",
    "\n",
    "output = []\n",
    "for dr in val_dirs:\n",
    "    curr_files = sorted(glob.glob(dr + '/img1/*.jpg'))\n",
    "    for f in curr_files:\n",
    "        output.append(f.replace('../data/', ''))\n",
    "             \n",
    "with open('./src/data/custom.val', 'w') as f:\n",
    "    for l in output:\n",
    "        f.writelines(l + '\\n')\n",
    "        \n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2019-11-24_Virginia_at_ArizonaState',\n",
       " '2019.01.14-Syracuse_at_Duke',\n",
       " '2019.03.02-NorthCarolina_at_Clemson',\n",
       " '2019.11.27-NorthCarolina_at_Alabama',\n",
       " '2019.12.04-OhioState_at_NorthCarolina',\n",
       " '2019.12.14-GeorgiaTech_at_Kentucky',\n",
       " '2020-02-08_Virginia_at_Louisville',\n",
       " '2020.01.14-Duke_at_Clemson',\n",
       " '2020.02.17-NorthCarolina_at_NotreDame',\n",
       " '2020.02.22-NorthCarolina_at_Louisville',\n",
       " '2020.02.22-Oregon_at_Arizona',\n",
       " '2020_01-20_NCState_at_Virginia'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_dirs = glob.glob('/media/data_disc/data/videos1/videos/*')\n",
    "global_dirs = sorted(global_dirs)\n",
    "global_dirs = [os.path.basename(x).replace('.mp4', '') for x in global_dirs]\n",
    "all_ = [os.path.basename(x) for x in all_dirs]\n",
    "(set(global_dirs) - set(all_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "cfg['root'] = '/home/ubuntu/oljike/PlayerTracking/data'\n",
    "cfg['train'] = {}\n",
    "cfg['train']['custom'] = './data/custom.train'\n",
    "cfg['test'] = {}\n",
    "cfg['test']['custom'] = './data/custom.val'\n",
    "cfg['test_emb'] = './data/custom.val'\n",
    "\n",
    "\n",
    "with open('src/lib/cfg/custom.json','w') as f:\n",
    "    json.dump(cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create team color labels and check them visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_team_color_labels(anno_dir, all_jsons):\n",
    "    all_labels = []\n",
    "    all_hists = []\n",
    "\n",
    "    \n",
    "    orig_dir = os.path.join('../../data/player_tracking_frames', os.path.basename(anno_dir))\n",
    "    for single_json in all_jsons:\n",
    "        data = json.load(open(single_json))\n",
    "\n",
    "        ### Read the image\n",
    "        img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "        img0 = cv2.imread(img_path)\n",
    "        h,w,_ = img0.shape\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            pts = np.array(data['shapes'][i]['points']).astype(int)\n",
    "\n",
    "            if pts[0][1] > pts[1][1] or pts[0][0] > pts[1][0]: continue\n",
    "            center_y = int((pts[1][1] + pts[0][1]) / 2)\n",
    "            center_x = int((pts[1][0] + pts[0][0]) / 2)\n",
    "\n",
    "            img_box = img0[max(0,center_y - 30): min(h, center_y + 30), \n",
    "                           max(0, center_x - 10): min(w, center_x + 10), :]\n",
    "            cv2.imwrite('small.jpg', img_box)\n",
    "\n",
    "            img_box = cv2.cvtColor(img_box, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            hist = cv2.calcHist([img_box], [0], None, [24],\n",
    "                                [0, 300])\n",
    "            hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "            all_hists.append(hist)\n",
    "            all_labels.append(data['shapes'][i]['label'])\n",
    "\n",
    "    concat_hists = np.concatenate(all_hists)\n",
    "    print(hist.shape)\n",
    "    km = KMeans(n_clusters=2, init=\"k-means++\", max_iter=10000).fit(all_hists)\n",
    "    print(Counter(km.labels_))\n",
    "    proc_cls, id_to_cls_val = post_process_cls(km.labels_, all_labels)\n",
    "    print(Counter(proc_cls))\n",
    "    return proc_cls, id_to_cls_val\n",
    "\n",
    "# anno_dir = glob.glob('../data/raw_data/*')[24]\n",
    "# all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "# proc_cls, id_to_cls_val = get_team_color_labels(anno_dir, all_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "h, w, _ = 720, 1280, 0\n",
    "out = cv2.VideoWriter('team_label_output.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 3, (w,h))\n",
    "\n",
    "orig_dir = os.path.join('../../data/player_tracking_frames', os.path.basename(anno_dir))\n",
    "\n",
    "en = 0\n",
    "for single_json in all_jsons:\n",
    "    data = json.load(open(single_json))\n",
    "    \n",
    "    ### Read the image\n",
    "    img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    for shape in data['shapes']:\n",
    "        bbox = np.array([[int(x) for x in y] for y in shape['points']])\n",
    "        \n",
    "        bbox = bbox.flatten()\n",
    "   \n",
    "        color = get_color(id_to_cls_val[shape['label']])\n",
    "        \n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, thickness=1)\n",
    "        cv2.putText(img, shape['label'], (bbox[0], max(0, bbox[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), thickness=1)\n",
    "       \n",
    "        en += 1\n",
    "    out.write(img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('team_label_output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visual check\n",
    "h, w, _ = 720, 1280, 0\n",
    "out = cv2.VideoWriter('team_label_output.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 3, (w,h))\n",
    "\n",
    "\n",
    "anno_dir = glob.glob('../data/raw_data/*')[1]\n",
    "all_jsons = sorted(glob.glob(anno_dir + '/*.json'))\n",
    "orig_dir = os.path.join('../../data/player_tracking_frames', os.path.basename(anno_dir))\n",
    "en = 0\n",
    "\n",
    "for single_json in all_jsons:\n",
    "    data = json.load(open(single_json))\n",
    "    \n",
    "    ### Read the image\n",
    "    img_path = os.path.join(orig_dir, os.path.basename(single_json).replace('.json', '.jpg')) \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    for shape in data['shapes']:\n",
    "        bbox = np.array([[int(x) for x in y] for y in shape['points']])\n",
    "        label = shape['label']\n",
    "        bbox = bbox.flatten()\n",
    "        track_label = id_dict[os.path.basename(anno_dir)][label]\n",
    "        player_lbl = id_to_cls_val[track_label]\n",
    "#         color = get_color(player_lbl)\n",
    "        \n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255,0,0), thickness=1)\n",
    "        cv2.putText(img, str(player_lbl), (bbox[0], max(0, bbox[1] - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), thickness=5)\n",
    "       \n",
    "        en += 1\n",
    "    out.write(img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"team_label_output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('team_label_output.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
